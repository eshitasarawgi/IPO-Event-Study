{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4450f0cf-4ea5-446b-b0db-901e388888cc",
   "metadata": {},
   "source": [
    "4. Data Loader and Builder\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3ffcc1-ccd3-4804-a666-85290ab3f612",
   "metadata": {},
   "source": [
    "4.1 MetaData Sourcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3b57ead0-6f33-454d-be70-696f3515b62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ IPO Metadata saved to: IPO_Meta\\ipo_metadata.csv\n",
      "✓ Total IPOs: 40\n",
      "✓ High Subscription: 20\n",
      "✓ Low Subscription: 20\n",
      "\n",
      "First 5 rows:\n",
      "              company_name         ticker listing_date subscription_category\n",
      "0         Mamata Machinery      MAMATA.NS   2024-12-27                  High\n",
      "1        Unimech Aerospace     UNIMECH.NS   2024-12-31                  High\n",
      "2         Mobikwik Systems    MOBIKWIK.NS   2024-12-18                  High\n",
      "3  Senores Pharmaceuticals     SENORES.NS   2024-12-30                  High\n",
      "4       Transrail Lighting  TRANSRAILL.NS   2024-12-27                  High\n",
      "\n",
      "Last 5 rows:\n",
      "           company_name        ticker listing_date subscription_category\n",
      "35                IRCTC      IRCTC.NS   2019-10-14                   Low\n",
      "36   Fino Payments Bank     FINOPB.NS   2021-10-29                   Low\n",
      "37  Indiamart Intermesh  INDIAMART.NS   2019-07-04                   Low\n",
      "38   Stanley Lifestyles    STANLEY.NS   2024-06-28                   Low\n",
      "39          Akums Drugs      AKUMS.NS   2024-07-31                   Low\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "IPO_META_FOLDER = os.path.join(\"IPO_Meta\")\n",
    "os.makedirs(IPO_META_FOLDER, exist_ok=True)\n",
    "IPO_METADATA_PATH = os.path.join(IPO_META_FOLDER, \"ipo_metadata.csv\")\n",
    "\n",
    "# ===== CORRECTED DATA: 40 Mainboard IPOs =====\n",
    "\n",
    "# HIGH SUBSCRIPTION IPOs (20)\n",
    "company_names_high = [\n",
    "    \"Mamata Machinery\", \"Unimech Aerospace\", \"Mobikwik Systems\", \"Senores Pharmaceuticals\",\n",
    "    \"Transrail Lighting\", \"Waaree Energies\", \"Tata Technologies\", \"Azad Engineering\",\n",
    "    \"Protean eGov Technologies\", \"Motisons Jewellers\", \"Jyoti CNC Automation\", \"DOMS Industries\",\n",
    "    \"Eternal\", \"Bharti Hexacom\", \"Suraj Estate Developers\", \"Paytm\",\n",
    "    \"Bajaj Housing Finance\", \"Awfis Space Solutions\", \"Aadhar Housing Finance\", \"Nykaa\"\n",
    "]\n",
    "\n",
    "tickers_high = [\n",
    "    \"MAMATA.NS\", \"UNIMECH.NS\", \"MOBIKWIK.NS\", \"SENORES.NS\",\n",
    "    \"TRANSRAILL.NS\", \"WAAREEENER.NS\", \"TATATECH.NS\", \"AZAD.NS\",\n",
    "    \"PROTEAN.BO\", \"MOTISONS.NS\", \"JYOTICNC.NS\", \"DOMS.NS\",\n",
    "    \"ETERNAL.NS\", \"BHARTIHEXA.NS\", \"SURAJEST.NS\", \"PAYTM.NS\",\n",
    "    \"BAJAJHFL.NS\", \"AWFIS.NS\", \"AADHARHFC.NS\", \"NYKAA.NS\"\n",
    "]\n",
    "\n",
    "listing_dates_high = [\n",
    "    \"2024-12-27\", \"2024-12-31\", \"2024-12-18\", \"2024-12-30\",\n",
    "    \"2024-12-27\", \"2024-10-28\", \"2023-11-30\", \"2024-01-19\",\n",
    "    \"2024-02-14\", \"2024-12-20\", \"2024-01-12\", \"2023-12-20\",\n",
    "    \"2021-07-23\", \"2024-04-12\", \"2024-06-14\", \"2021-11-18\",\n",
    "    \"2024-09-16\", \"2024-05-30\", \"2024-05-15\", \"2021-11-10\"\n",
    "]\n",
    "\n",
    "# LOW SUBSCRIPTION IPOs (20)\n",
    "company_names_low = [\n",
    "    \"Hyundai Motor India\", \"Swiggy\", \"Sagility India\", \"Niva Bupa Health Insurance\",\n",
    "    \"Ola Electric Mobility\", \"Delhivery\", \"EaseMyTrip\", \"Carraro India\",\n",
    "    \"Suraksha Diagnostic\", \"Indo Farm Equipment\", \"CarTrade Tech\", \"Sanathan Textiles\",\n",
    "    \"Nazara Tech\", \"Latent View\", \"Le Travenues Technology\", \"IRCTC\",\n",
    "    \"Fino Payments Bank\", \"Indiamart Intermesh\", \"Stanley Lifestyles\", \"Akums Drugs\"\n",
    "]\n",
    "\n",
    "tickers_low = [\n",
    "    \"HYUNDAI.NS\", \"SWIGGY.NS\", \"SAGILITY.NS\", \"NIVABUPA.NS\",\n",
    "    \"OLAELEC.NS\", \"DELHIVERY.NS\", \"EASEMYTRIP.NS\", \"CARRARO.NS\",\n",
    "    \"SURAKSHA.NS\", \"INDOFARM.NS\", \"CARTRADE.NS\", \"SANATHAN.NS\",\n",
    "    \"NAZARA.NS\", \"LATENTVIEW.NS\", \"IXIGO.NS\", \"IRCTC.NS\",\n",
    "    \"FINOPB.NS\", \"INDIAMART.NS\", \"STANLEY.NS\", \"AKUMS.NS\"\n",
    "]\n",
    "\n",
    "listing_dates_low = [\n",
    "    \"2024-10-22\", \"2024-11-13\", \"2024-11-11\", \"2024-11-11\",\n",
    "    \"2024-08-09\", \"2022-05-24\", \"2021-03-19\", \"2024-12-24\",\n",
    "    \"2024-12-06\", \"2024-12-31\", \"2021-08-20\", \"2024-12-27\",\n",
    "    \"2021-03-30\", \"2021-11-23\", \"2024-06-18\", \"2019-10-14\",\n",
    "    \"2021-10-29\", \"2019-07-04\", \"2024-06-28\", \"2024-07-31\"\n",
    "]\n",
    "\n",
    "# Combine both samples\n",
    "company_names = company_names_high + company_names_low\n",
    "tickers = tickers_high + tickers_low\n",
    "listing_dates = listing_dates_high + listing_dates_low\n",
    "subscription_category = [\"High\"] * 20 + [\"Low\"] * 20\n",
    "\n",
    "# Create DataFrame\n",
    "df_meta = pd.DataFrame({\n",
    "    \"company_name\": company_names,\n",
    "    \"ticker\": tickers,\n",
    "    \"listing_date\": listing_dates,\n",
    "    \"subscription_category\": subscription_category\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "df_meta.to_csv(IPO_METADATA_PATH, index=False)\n",
    "\n",
    "print(f\"✓ IPO Metadata saved to: {IPO_METADATA_PATH}\")\n",
    "print(f\"✓ Total IPOs: {len(df_meta)}\")\n",
    "print(f\"✓ High Subscription: {len(df_meta[df_meta['subscription_category'] == 'High'])}\")\n",
    "print(f\"✓ Low Subscription: {len(df_meta[df_meta['subscription_category'] == 'Low'])}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df_meta.head())\n",
    "print(\"\\nLast 5 rows:\")\n",
    "print(df_meta.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbadaa0-f64e-43b9-a760-f6a8e5b8fbc4",
   "metadata": {},
   "source": [
    "4.2 Imports & Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "828d1908-acbe-416b-adf0-60e311471efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section 4.2 Loaded Successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "\n",
    "#Suppressing warnings here to keep notebook output clean\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Displaying Settings for Clean Tables\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 50)\n",
    "\n",
    "print (\"Section 4.2 Loaded Successfully\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af94b7f-7abc-47d5-b443-047259ce5213",
   "metadata": {},
   "source": [
    "4.3 IPO MetaData Loading\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dc803ca9-9be2-4755-a060-cc79326f0400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPO metadata loaded successfully.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_name</th>\n",
       "      <th>ticker</th>\n",
       "      <th>listing_date</th>\n",
       "      <th>subscription_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mamata Machinery</td>\n",
       "      <td>MAMATA.NS</td>\n",
       "      <td>2024-12-27</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Unimech Aerospace</td>\n",
       "      <td>UNIMECH.NS</td>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mobikwik Systems</td>\n",
       "      <td>MOBIKWIK.NS</td>\n",
       "      <td>2024-12-18</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Senores Pharmaceuticals</td>\n",
       "      <td>SENORES.NS</td>\n",
       "      <td>2024-12-30</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Transrail Lighting</td>\n",
       "      <td>TRANSRAILL.NS</td>\n",
       "      <td>2024-12-27</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              company_name         ticker listing_date subscription_category\n",
       "0         Mamata Machinery      MAMATA.NS   2024-12-27                  High\n",
       "1        Unimech Aerospace     UNIMECH.NS   2024-12-31                  High\n",
       "2         Mobikwik Systems    MOBIKWIK.NS   2024-12-18                  High\n",
       "3  Senores Pharmaceuticals     SENORES.NS   2024-12-30                  High\n",
       "4       Transrail Lighting  TRANSRAILL.NS   2024-12-27                  High"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating required columns...\n",
      "\n",
      "All required columns present.\n",
      "\n",
      "Section 4.3 completed successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_name</th>\n",
       "      <th>ticker</th>\n",
       "      <th>listing_date</th>\n",
       "      <th>subscription_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Indiamart Intermesh</td>\n",
       "      <td>INDIAMART.NS</td>\n",
       "      <td>2019-07-04</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IRCTC</td>\n",
       "      <td>IRCTC.NS</td>\n",
       "      <td>2019-10-14</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EaseMyTrip</td>\n",
       "      <td>EASEMYTRIP.NS</td>\n",
       "      <td>2021-03-19</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nazara Tech</td>\n",
       "      <td>NAZARA.NS</td>\n",
       "      <td>2021-03-30</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Eternal</td>\n",
       "      <td>ETERNAL.NS</td>\n",
       "      <td>2021-07-23</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          company_name         ticker listing_date subscription_category\n",
       "0  Indiamart Intermesh   INDIAMART.NS   2019-07-04                   Low\n",
       "1                IRCTC       IRCTC.NS   2019-10-14                   Low\n",
       "2           EaseMyTrip  EASEMYTRIP.NS   2021-03-19                   Low\n",
       "3          Nazara Tech      NAZARA.NS   2021-03-30                   Low\n",
       "4              Eternal     ETERNAL.NS   2021-07-23                  High"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load metadata\n",
    "try:\n",
    "    ipo_df = pd.read_csv(IPO_METADATA_PATH)\n",
    "    print(\"IPO metadata loaded successfully.\\n\")\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found. Please check path and load again.\")\n",
    "    raise\n",
    "\n",
    "# Preview data\n",
    "display(ipo_df.head())\n",
    "\n",
    "# Validating essential columns\n",
    "required_cols = [\"company_name\", \"ticker\", \"listing_date\", \"subscription_category\"]\n",
    "\n",
    "\n",
    "print(\"\\nValidating required columns...\\n\")\n",
    "missing_cols = [c for c in required_cols if c not in ipo_df.columns]\n",
    "\n",
    "if missing_cols:\n",
    "    print(\"Missing columns:\", missing_cols)\n",
    "    raise Exception(\"Metadata file incomplete. Fix before proceeding.\")\n",
    "\n",
    "print(\"All required columns present.\\n\")\n",
    "\n",
    "# Convert date column into proper datetime format\n",
    "ipo_df[\"listing_date\"] = pd.to_datetime(ipo_df[\"listing_date\"])\n",
    "\n",
    "# Sort by listing date for cleaner processing\n",
    "ipo_df = ipo_df.sort_values(\"listing_date\").reset_index(drop=True)\n",
    "\n",
    "print(\"Section 4.3 completed successfully.\")\n",
    "display(ipo_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bc94b3c3-72f4-4f32-9297-47c58caebcb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata path (expected): ..\\Notebooks\\IPO_Meta\\ipo_metadata.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Paths (update only if you changed folder names)\n",
    "META_PATH = os.path.join(\"..\", \"Notebooks\", \"IPO_Meta\", \"ipo_metadata.csv\")   # <-- metadata CSV you created\n",
    "OUT_PROCESSED = os.path.join(\"..\", \"Notebooks\", \"IPO_Meta\",\"meta_output_processed.csv\")\n",
    "os.makedirs(OUT_PROCESSED, exist_ok=True)\n",
    "\n",
    "OUT_DAILY_MASTER = os.path.join(OUT_PROCESSED, \"df_stock_master.csv\")\n",
    "OUT_POOLED_MASTER = os.path.join(OUT_PROCESSED, \"df_pooled_master.csv\")\n",
    "OUT_SAMPLE1 = os.path.join(OUT_PROCESSED, \"sample1_final.csv\")\n",
    "OUT_SAMPLE2 = os.path.join(OUT_PROCESSED, \"sample2_final.csv\")\n",
    "\n",
    "# Global fetch window (broad enough to cover event windows)\n",
    "GLOBAL_START = \"2019-01-01\"\n",
    "GLOBAL_END   = \"2025-02-15\"\n",
    "EVENT_WINDOW_DAYS = 30  # 30-day post-listing as decided \n",
    "print(\"Metadata path (expected):\", META_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed277cae-313b-479c-90fd-87b1927d84d8",
   "metadata": {},
   "source": [
    "4.4 Data Loader Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "11e6f217-20c7-4ec8-adc5-a8fcbc823498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Fetch data for a single stock\n",
    "def fetch_stock_data(ticker, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch historical OHLCV data for a single ticker.\n",
    "    Includes error handling so pipeline never breaks.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = yf.download(ticker, start=start_date, end=end_date, progress=False, threads=False)\n",
    "\n",
    "        if df is None or df.empty:\n",
    "            print(f\" Warning: No data found for {ticker}.\")\n",
    "            return None\n",
    "\n",
    "        df = df.reset_index()\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Error fetching {ticker}: {e}\")\n",
    "        return None\n",
    "\n",
    " \n",
    "# 2. Fetch benchmark (NIFTY)\n",
    " \n",
    "def fetch_benchmark(start_date, end_date, index_ticker=\"^NSEI\"):\n",
    "    \"\"\"\n",
    "    Fetch benchmark index used for CAR calculations.\n",
    "    Using NIFTY (^NSEI).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = yf.download(index_ticker, start=start_date, end=end_date, progress=False, threads=False)\n",
    "\n",
    "        if df is None or df.empty:\n",
    "            print(\" Warning: No benchmark data found.\")\n",
    "            return None\n",
    "\n",
    "        df = df.reset_index()\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Error fetching benchmark: {e}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5456d437-0a90-43a0-bff7-f02fbd3c553f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SOLUTION FOR: YFTzMissingError('possibly delisted; no timezone found')\n",
    "\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "def fetch_stock_data_robust(ticker, start_date, end_date, max_retries=2):\n",
    "    \"\"\"\n",
    "    Robust stock data fetcher with multiple fallback strategies.\n",
    "    \n",
    "    Strategy:\n",
    "    1. Try NSE ticker (.NS)\n",
    "    2. If fails, try BSE ticker (.BO)\n",
    "    3. If still fails, use alternative download parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    attempts = [\n",
    "        ticker,  # Original ticker\n",
    "        ticker.replace('.NS', '.BO'),  # Try BSE if NSE fails\n",
    "    ]\n",
    "    \n",
    "    for attempt_ticker in attempts:\n",
    "        for retry in range(max_retries):\n",
    "            try:\n",
    "                # Method 1: Standard download\n",
    "                df = yf.download(\n",
    "                    attempt_ticker, \n",
    "                    start=start_date, \n",
    "                    end=end_date, \n",
    "                    progress=False,\n",
    "                    auto_adjust=False,  # Explicitly set to False for consistency\n",
    "                    threads=False\n",
    "                )\n",
    "                \n",
    "                if df is not None and not df.empty:\n",
    "                    df = df.reset_index()\n",
    "                    print(f\"✓ Successfully fetched: {attempt_ticker}\")\n",
    "                    return df\n",
    "                    \n",
    "            except Exception as e:\n",
    "                if \"YFTzMissingError\" in str(e) or \"timezone\" in str(e).lower():\n",
    "                    # Try alternative method for timezone issues\n",
    "                    try:\n",
    "                        stock = yf.Ticker(attempt_ticker)\n",
    "                        df = stock.history(start=start_date, end=end_date, auto_adjust=False)\n",
    "                        if not df.empty:\n",
    "                            df = df.reset_index()\n",
    "                            print(f\"✓ Fetched via .history(): {attempt_ticker}\")\n",
    "                            return df\n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "                if retry == max_retries - 1 and attempt_ticker == attempts[-1]:\n",
    "                    print(f\"❌ All attempts failed for {ticker}: {str(e)[:100]}\")\n",
    "                    \n",
    "    return None\n",
    "\n",
    "\n",
    "def fetch_benchmark_robust(start_date, end_date, index_ticker=\"^NSEI\"):\n",
    "    \"\"\"\n",
    "    Robust benchmark fetcher with fallback to BSE Sensex if NIFTY fails.\n",
    "    \"\"\"\n",
    "    indexes = [index_ticker, \"^BSESN\"]  # NIFTY 50, then BSE Sensex\n",
    "    \n",
    "    for idx in indexes:\n",
    "        try:\n",
    "            df = yf.download(idx, start=start_date, end=end_date, progress=False, threads=False)\n",
    "            if df is not None and not df.empty:\n",
    "                df = df.reset_index()\n",
    "                print(f\"✓ Benchmark loaded: {idx}\")\n",
    "                return df\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ {idx} failed: {str(e)[:50]}\")\n",
    "            continue\n",
    "    \n",
    "    raise Exception(\"Could not fetch any benchmark index (NIFTY or Sensex)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ac85b01f-ad52-4e2c-a33b-24b9c1c45243",
   "metadata": {},
   "outputs": [],
   "source": [
    "TICKER_CORRECTIONS = {\n",
    "    # Wrong -> Correct mapping\n",
    "    'TRANSRAIL.NS': 'TRANSRAILL.NS',  # Double L\n",
    "    'WAAREE.NS': 'WAAREEENER.NS',     # Full company name\n",
    "    'PROTEAN.NS': 'PROTEAN.BO'       # BSE works better\n",
    "   }\n",
    "\n",
    "def apply_ticker_corrections(ticker):\n",
    "    \"\"\"Apply known ticker corrections before fetching.\"\"\"\n",
    "    return TICKER_CORRECTIONS.get(ticker, ticker)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c78bc04-a7df-4349-826c-0fc6be0990d0",
   "metadata": {},
   "source": [
    "4.5 Small Utility Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f0ffaf0e-c214-4b5b-83cf-d0fdfc377ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_multiindex_columns_if_needed(df):\n",
    "    \"\"\"If df has multiindex columns, flatten to single level.\"\"\"\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        # Take only the first level (the actual column names)\n",
    "        df.columns = df.columns.get_level_values(0)\n",
    "    return df\n",
    "\n",
    "def pick_price_column_and_rename(df, target_name=\"Adj_Close\"):\n",
    "    # Check for exact matches first\n",
    "    if \"Adj Close\" in df.columns:\n",
    "        df = df.rename(columns={\"Adj Close\": target_name})\n",
    "        return df\n",
    "    elif \"Close\" in df.columns:\n",
    "        df = df.rename(columns={\"Close\": target_name})\n",
    "        return df\n",
    "    \n",
    "    # Fallback: search for columns containing these terms\n",
    "    adj_cols = [c for c in df.columns if \"adj\" in str(c).lower() and \"close\" in str(c).lower()]\n",
    "    close_cols = [c for c in df.columns if \"close\" in str(c).lower()]\n",
    "    \n",
    "    if adj_cols:\n",
    "        df = df.rename(columns={adj_cols[0]: target_name})\n",
    "        return df\n",
    "    elif close_cols:\n",
    "        df = df.rename(columns={close_cols[0]: target_name})\n",
    "        return df\n",
    "    else:\n",
    "        raise KeyError(f\"No price column (Adj Close / Close) found. Columns: {list(df.columns)}\")\n",
    "\n",
    "def event_window_bounds(listing_date, pre_days=7 , post_days= EVENT_WINDOW_DAYS):\n",
    "    #Using a small buffer (pre_days and post_days) to ensure we capture trading days.\n",
    "    start = (listing_date - timedelta(days=pre_days)).strftime(\"%Y-%m-%d\")\n",
    "    end = (listing_date + timedelta(days = post_days + pre_days)).strftime(\"%Y-%m-%d\")\n",
    "    return start, end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c91d47-aa4b-4298-9cc8-b6412f4ea145",
   "metadata": {},
   "source": [
    "4.6 Loading MetaData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c217e693-77c8-4773-ba75-b62957900a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded metadata. Total rows: 40\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(META_PATH):\n",
    "    raise FileNotFoundError(f\"MetaData file not found at {META_PATH}.create it first and re-run\")\n",
    "\n",
    "ipo_meta = pd.read_csv(META_PATH)\n",
    "ipo_meta.columns = [c.strip() for c in ipo_meta.columns]\n",
    "if \"subscription_category\" in ipo_meta.columns and \"subscription_cat\" not in ipo_meta.columns:\n",
    "    ipo_meta = ipo_meta.rename(columns={\"subscription_category\" : \"subscription_cat\"})\n",
    "\n",
    "#Ensuring Required Columns Exist\n",
    "required = [\"company_name\", \"ticker\", \"listing_date\", \"subscription_cat\"]\n",
    "missing = [c for c in required if c not in ipo_meta.columns]\n",
    "if missing:\n",
    "    raise Exception(f\"MetaData missing required column: {missing}. Add them and re-run.\")\n",
    "\n",
    "ipo_meta[\"listing_date\"] = pd.to_datetime(ipo_meta[\"listing_date\"])\n",
    "print(\"Loaded metadata. Total rows:\", len(ipo_meta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0963281-5dfe-484b-8acf-6c896ebb1dc4",
   "metadata": {},
   "source": [
    "4.7 Preload Benchmark NIFTY\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e5f48f7e-21ce-4c8c-a8a9-f506cbd1cdea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Benchmark loaded: ^NSEI\n",
      "NIFTY Prepared. Rows:  1512\n"
     ]
    }
   ],
   "source": [
    "nifty_full = fetch_benchmark_robust(GLOBAL_START, GLOBAL_END)\n",
    "if nifty_full is None:\n",
    "    raise Exception(\"Failed to fetch benchmark data (NIFTY)\")\n",
    "\n",
    "nifty_full = flatten_multiindex_columns_if_needed(nifty_full)\n",
    "\n",
    "if \"Date\" not in nifty_full.columns and \"date\" not in nifty_full.columns:\n",
    "    nifty_full = nifty_full.reset_index()\n",
    "\n",
    "date_cols= [c for c in nifty_full.columns if \"date\" in c.lower()]\n",
    "if not date_cols:\n",
    "    raise Exception(\"Could not find a date column in NIFTY Data\")\n",
    "\n",
    "#price column\n",
    "price_col = None\n",
    "if \"Adj Close\" in nifty_full.columns:\n",
    "    price_col = \"Adj Close\"\n",
    "elif \"Close\" in nifty_full.columns:\n",
    "    price_col = \"Close\"\n",
    "else: \n",
    "    raise Exception(f\"No price column in NIFTY. Available: {nifty_full.columns.tolist()}\")\n",
    "\n",
    "#Selecting only needed columns and using direct assignment\n",
    "nifty_full = nifty_full[[date_cols[0], price_col]].copy()\n",
    "nifty_full.columns = [\"date\", \"Adj_Close\"]\n",
    "\n",
    "nifty_full[\"date\"] = pd.to_datetime(nifty_full[\"date\"])\n",
    "nifty_full = nifty_full.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "#Computation of NIFTY Returns\n",
    "nifty_full[\"nifty_return\"] = nifty_full[\"Adj_Close\"].pct_change()\n",
    "\n",
    "print (\"NIFTY Prepared. Rows: \", len(nifty_full))\n",
    "\n",
    "                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "acb85b49-7bb1-41cb-8206-2b55d338ffec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NIFTY saved: IPO_Meta\\nifty_final.csv\n"
     ]
    }
   ],
   "source": [
    "#Creation of NIFTY CSV\n",
    "nifty_output_path = os.path.join(IPO_META_FOLDER, \"nifty_final.csv\")\n",
    "nifty_full.to_csv(nifty_output_path, index=False)\n",
    "print(\"NIFTY saved:\", nifty_output_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "edbd6e51-f7ec-464d-92a4-22c2e6711586",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Containers For Results\n",
    "daily_rows = []\n",
    "ipo_summary = []\n",
    "failed_tickers = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558b0b14-84f4-45e0-be92-6601131653f1",
   "metadata": {},
   "source": [
    "4.8 Helper Metric Functions (Operate on merged df indexed by Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a53bb43b-1e7d-4d74-a394-1d733f787719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_car30(merged_df, listing_first_date):\n",
    "    \"\"\"\n",
    "    merged_df must contain columns: 'date', 'abnormal_return', 'days_from_listing'\n",
    "    Returns CAR at day 30 (or last available <=30) as float or NaN.\n",
    "    \"\"\"\n",
    "    window = merged_df[(merged_df[\"days_from_listing\"] >= 0) & (merged_df[\"days_from_listing\"] <= EVENT_WINDOW_DAYS)]\n",
    "    if window.empty:\n",
    "        return np.nan\n",
    "    # cumulative abnormal return from day 0 onwards\n",
    "    window = window.sort_values(\"days_from_listing\")\n",
    "    car_series = window[\"abnormal_return\"].fillna(0).cumsum()\n",
    "    # take value at day 30 if exists else last available <=30\n",
    "    if (window[\"days_from_listing\"] == EVENT_WINDOW_DAYS).any():\n",
    "        return float(car_series.iloc[(window[\"days_from_listing\"] == EVENT_WINDOW_DAYS).values.argmax()])\n",
    "    else:\n",
    "        return float(car_series.iloc[-1])\n",
    "\n",
    "def compute_day1_return(merged_df):\n",
    "    row = merged_df[merged_df[\"days_from_listing\"] == 1]\n",
    "    if row.empty:\n",
    "        return np.nan\n",
    "    return float(row[\"ipo_return\"].iloc[0])\n",
    "\n",
    "def compute_vol30(merged_df):\n",
    "    window = merged_df[(merged_df[\"days_from_listing\"] >= 1) & (merged_df[\"days_from_listing\"] <= EVENT_WINDOW_DAYS)]\n",
    "    if window.empty:\n",
    "        return np.nan\n",
    "    return float(window[\"ipo_return\"].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76f510a-8e84-4e28-a56b-e51f95b2208a",
   "metadata": {},
   "source": [
    "4.9 Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e6be3032-2d93-47aa-a979-43cbe7e7f428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Mamata Machinery (MAMATA.NS) ...\n",
      "✓ Successfully fetched: MAMATA.NS\n",
      " ✓ OK: MAMATA.NS | CAR30=-0.3334 | vol30=0.06264024202847081\n",
      "\n",
      "Processing Unimech Aerospace (UNIMECH.NS) ...\n",
      "✓ Successfully fetched: UNIMECH.NS\n",
      " ✓ OK: UNIMECH.NS | CAR30=-0.0484 | vol30=0.034314914824892545\n",
      "\n",
      "Processing Mobikwik Systems (MOBIKWIK.NS) ...\n",
      "✓ Successfully fetched: MOBIKWIK.NS\n",
      " ✓ OK: MOBIKWIK.NS | CAR30=-0.0290 | vol30=0.06582373315385136\n",
      "\n",
      "Processing Senores Pharmaceuticals (SENORES.NS) ...\n",
      "✓ Successfully fetched: SENORES.NS\n",
      " ✓ OK: SENORES.NS | CAR30=-0.0362 | vol30=0.048853728356994114\n",
      "\n",
      "Processing Transrail Lighting (TRANSRAILL.NS) ...\n",
      "✓ Successfully fetched: TRANSRAILL.NS\n",
      " ✓ OK: TRANSRAILL.NS | CAR30=0.0065 | vol30=0.050415761633091555\n",
      "\n",
      "Processing Waaree Energies (WAAREEENER.NS) ...\n",
      "✓ Successfully fetched: WAAREEENER.NS\n",
      " ✓ OK: WAAREEENER.NS | CAR30=0.1518 | vol30=0.06630510393593858\n",
      "\n",
      "Processing Tata Technologies (TATATECH.NS) ...\n",
      "✓ Successfully fetched: TATATECH.NS\n",
      " ✓ OK: TATATECH.NS | CAR30=-0.1791 | vol30=0.021016460070018495\n",
      "\n",
      "Processing Azad Engineering (AZAD.NS) ...\n",
      "✓ Successfully fetched: AZAD.NS\n",
      " ✓ OK: AZAD.NS | CAR30=0.5230 | vol30=0.059887705491584405\n",
      "\n",
      "Processing Protean eGov Technologies (PROTEAN.BO) ...\n",
      "✓ Successfully fetched: PROTEAN.BO\n",
      " ✓ OK: PROTEAN.BO | CAR30=-0.2693 | vol30=0.01887088497718942\n",
      "\n",
      "Processing Motisons Jewellers (MOTISONS.NS) ...\n",
      "✓ Successfully fetched: MOTISONS.NS\n",
      " ✓ OK: MOTISONS.NS | CAR30=-0.0557 | vol30=0.02882286790899594\n",
      "\n",
      "Processing Jyoti CNC Automation (JYOTICNC.NS) ...\n",
      "✓ Successfully fetched: JYOTICNC.NS\n",
      " ✓ OK: JYOTICNC.NS | CAR30=0.4122 | vol30=0.06181985372373079\n",
      "\n",
      "Processing DOMS Industries (DOMS.NS) ...\n",
      "✓ Successfully fetched: DOMS.NS\n",
      " ✓ OK: DOMS.NS | CAR30=0.0642 | vol30=0.031174316003156684\n",
      "\n",
      "Processing Eternal (ETERNAL.NS) ...\n",
      "✓ Successfully fetched: ETERNAL.NS\n",
      " ✓ OK: ETERNAL.NS | CAR30=0.0828 | vol30=0.0470744264379534\n",
      "\n",
      "Processing Bharti Hexacom (BHARTIHEXA.NS) ...\n",
      "✓ Successfully fetched: BHARTIHEXA.NS\n",
      " ✓ OK: BHARTIHEXA.NS | CAR30=0.1370 | vol30=0.038666796468214686\n",
      "\n",
      "Processing Suraj Estate Developers (SURAJEST.NS) ...\n",
      "✓ Successfully fetched: SURAJEST.NS\n",
      " ✓ OK: SURAJEST.NS | CAR30=0.3742 | vol30=0.046245305710778895\n",
      "\n",
      "Processing Paytm (PAYTM.NS) ...\n",
      "✓ Successfully fetched: PAYTM.NS\n",
      " ✓ OK: PAYTM.NS | CAR30=-0.0872 | vol30=0.062002674525566515\n",
      "\n",
      "Processing Bajaj Housing Finance (BAJAJHFL.NS) ...\n",
      "✓ Successfully fetched: BAJAJHFL.NS\n",
      " ✓ OK: BAJAJHFL.NS | CAR30=-0.1215 | vol30=0.04768896638550182\n",
      "\n",
      "Processing Awfis Space Solutions (AWFIS.NS) ...\n",
      "✓ Successfully fetched: AWFIS.NS\n",
      " ✓ OK: AWFIS.NS | CAR30=0.1467 | vol30=0.04042539071327936\n",
      "\n",
      "Processing Aadhar Housing Finance (AADHARHFC.NS) ...\n",
      "✓ Successfully fetched: AADHARHFC.NS\n",
      " ✓ OK: AADHARHFC.NS | CAR30=0.1973 | vol30=0.03435267853398316\n",
      "\n",
      "Processing Nykaa (NYKAA.NS) ...\n",
      "✓ Successfully fetched: NYKAA.NS\n",
      " ✓ OK: NYKAA.NS | CAR30=-0.0029 | vol30=0.03498784726263869\n",
      "\n",
      "Processing Hyundai Motor India (HYUNDAI.NS) ...\n",
      "✓ Successfully fetched: HYUNDAI.NS\n",
      " ✓ OK: HYUNDAI.NS | CAR30=0.0551 | vol30=0.02406882127600798\n",
      "\n",
      "Processing Swiggy (SWIGGY.NS) ...\n",
      "✓ Successfully fetched: SWIGGY.NS\n",
      " ✓ OK: SWIGGY.NS | CAR30=0.1191 | vol30=0.03929599376159756\n",
      "\n",
      "Processing Sagility India (SAGILITY.NS) ...\n",
      "✓ Successfully fetched: SAGILITY.NS\n",
      " ✓ OK: SAGILITY.NS | CAR30=0.3204 | vol30=0.04053217591379209\n",
      "\n",
      "Processing Niva Bupa Health Insurance (NIVABUPA.NS) ...\n",
      "✓ Successfully fetched: NIVABUPA.NS\n",
      " ✓ OK: NIVABUPA.NS | CAR30=0.1000 | vol30=0.05965162165605839\n",
      "\n",
      "Processing Ola Electric Mobility (OLAELEC.NS) ...\n",
      "✓ Successfully fetched: OLAELEC.NS\n",
      " ✓ OK: OLAELEC.NS | CAR30=0.2118 | vol30=0.07576552647311482\n",
      "\n",
      "Processing Delhivery (DELHIVERY.NS) ...\n",
      "✓ Successfully fetched: DELHIVERY.NS\n",
      " ✓ OK: DELHIVERY.NS | CAR30=-0.0170 | vol30=0.040983551056295434\n",
      "\n",
      "Processing EaseMyTrip (EASEMYTRIP.NS) ...\n",
      "✓ Successfully fetched: EASEMYTRIP.NS\n",
      " ✓ OK: EASEMYTRIP.NS | CAR30=-0.1488 | vol30=0.059521525041175964\n",
      "\n",
      "Processing Carraro India (CARRARO.NS) ...\n",
      "✓ Successfully fetched: CARRARO.NS\n",
      " ✓ OK: CARRARO.NS | CAR30=-0.1993 | vol30=0.026378517787839905\n",
      "\n",
      "Processing Suraksha Diagnostic (SURAKSHA.NS) ...\n",
      "✓ Successfully fetched: SURAKSHA.NS\n",
      " ✓ OK: SURAKSHA.NS | CAR30=-0.0688 | vol30=0.01486166000883245\n",
      "\n",
      "Processing Indo Farm Equipment (INDOFARM.NS) ...\n",
      "✓ Successfully fetched: INDOFARM.NS\n",
      " ✓ OK: INDOFARM.NS | CAR30=-0.2803 | vol30=0.056640798685221094\n",
      "\n",
      "Processing CarTrade Tech (CARTRADE.NS) ...\n",
      "✓ Successfully fetched: CARTRADE.NS\n",
      " ✓ OK: CARTRADE.NS | CAR30=-0.0952 | vol30=0.024977629820355424\n",
      "\n",
      "Processing Sanathan Textiles (SANATHAN.NS) ...\n",
      "✓ Successfully fetched: SANATHAN.NS\n",
      " ✓ OK: SANATHAN.NS | CAR30=-0.0902 | vol30=0.02985378268294972\n",
      "\n",
      "Processing Nazara Tech (NAZARA.NS) ...\n",
      "✓ Successfully fetched: NAZARA.NS\n",
      " ✓ OK: NAZARA.NS | CAR30=0.1136 | vol30=0.052245850372556885\n",
      "\n",
      "Processing Latent View (LATENTVIEW.NS) ...\n",
      "✓ Successfully fetched: LATENTVIEW.NS\n",
      " ✓ OK: LATENTVIEW.NS | CAR30=0.0902 | vol30=0.07135212182058315\n",
      "\n",
      "Processing Le Travenues Technology (IXIGO.NS) ...\n",
      "✓ Successfully fetched: IXIGO.NS\n",
      " ✓ OK: IXIGO.NS | CAR30=0.0066 | vol30=0.03835086094053084\n",
      "\n",
      "Processing IRCTC (IRCTC.NS) ...\n",
      "✓ Successfully fetched: IRCTC.NS\n",
      " ✓ OK: IRCTC.NS | CAR30=0.2060 | vol30=0.0331976372536121\n",
      "\n",
      "Processing Fino Payments Bank (FINOPB.NS) ...\n",
      "✓ Successfully fetched: FINOPB.NS\n",
      " ✓ OK: FINOPB.NS | CAR30=-0.1729 | vol30=0.06292306875785597\n",
      "\n",
      "Processing Indiamart Intermesh (INDIAMART.NS) ...\n",
      "✓ Successfully fetched: INDIAMART.NS\n",
      " ✓ OK: INDIAMART.NS | CAR30=0.0279 | vol30=0.017292668244550245\n",
      "\n",
      "Processing Stanley Lifestyles (STANLEY.NS) ...\n",
      "✓ Successfully fetched: STANLEY.NS\n",
      " ✓ OK: STANLEY.NS | CAR30=0.1747 | vol30=0.04178324070704321\n",
      "\n",
      "Processing Akums Drugs (AKUMS.NS) ...\n",
      "✓ Successfully fetched: AKUMS.NS\n",
      " ✓ OK: AKUMS.NS | CAR30=0.0789 | vol30=0.04705669486053886\n"
     ]
    }
   ],
   "source": [
    "for idx, meta_row in ipo_meta.iterrows():\n",
    "    company = meta_row[\"company_name\"]\n",
    "    ticker = meta_row[\"ticker\"]\n",
    "    ticker = apply_ticker_corrections(ticker)\n",
    "    listing_date = meta_row[\"listing_date\"]\n",
    "\n",
    "    print(f\"\\nProcessing {company} ({ticker}) ...\")\n",
    "\n",
    "    # If listing_date is NaT, skip for now\n",
    "    if pd.isna(listing_date):\n",
    "        print(f\" - SKIP: listing_date is NaT for {company}\")\n",
    "        failed_tickers.append({\"company\": company, \"ticker\": ticker, \"reason\": \"listing_date_NaT\"})\n",
    "        continue\n",
    "\n",
    "    # Build fetch window (with buffer)\n",
    "    start_date, end_date = event_window_bounds(listing_date, pre_days=7, post_days=EVENT_WINDOW_DAYS)\n",
    "\n",
    "    # Fetch stock data\n",
    "    stock_df = fetch_stock_data_robust(ticker, start_date, end_date)\n",
    "    if stock_df is None or stock_df.empty:\n",
    "        print(f\" - SKIP: No stock data for {ticker}\")\n",
    "        failed_tickers.append({\"company\": company, \"ticker\": ticker, \"reason\": \"no_stock_data\"})\n",
    "        continue\n",
    "\n",
    "    # Flatten MultiIndex if present\n",
    "    stock_df = flatten_multiindex_columns_if_needed(stock_df)\n",
    "\n",
    "    # Ensure Date column exists\n",
    "    if \"Date\" not in stock_df.columns and \"date\" not in stock_df.columns:\n",
    "        stock_df = stock_df.reset_index()\n",
    "    \n",
    "    # Find date column\n",
    "    stock_date_cols = [c for c in stock_df.columns if \"date\" in c.lower()]\n",
    "    if not stock_date_cols:\n",
    "        print(f\" - SKIP: No date column for {ticker}\")\n",
    "        failed_tickers.append({\"company\": company, \"ticker\": ticker, \"reason\": \"no_date_column\"})\n",
    "        continue\n",
    "    \n",
    "    stock_df = stock_df.rename(columns={stock_date_cols[0]: \"date\"})\n",
    "    stock_df[\"date\"] = pd.to_datetime(stock_df[\"date\"], errors=\"coerce\")\n",
    "    \n",
    "    if stock_df[\"date\"].isna().all():\n",
    "        print(f\" - SKIP: all dates NaT for {ticker}\")\n",
    "        failed_tickers.append({\"company\": company, \"ticker\": ticker, \"reason\": \"all_dates_NaT\"})\n",
    "        continue\n",
    "\n",
    "    # Find and normalize price column\n",
    "    try:\n",
    "        stock_price_col = None\n",
    "        if \"Adj Close\" in stock_df.columns:\n",
    "            stock_price_col = \"Adj Close\"\n",
    "        elif \"Close\" in stock_df.columns:\n",
    "            stock_price_col = \"Close\"\n",
    "        else:\n",
    "            raise KeyError(\"No price column\")\n",
    "        \n",
    "        # Add ticker column before selection\n",
    "        stock_df[\"ticker\"] = ticker\n",
    "        \n",
    "        # Select and rename using direct assignment\n",
    "        stock_df = stock_df[[\"date\", stock_price_col, \"ticker\"]].copy()\n",
    "        stock_df.columns = [\"date\", \"Adj_Close\", \"ticker\"]\n",
    "        \n",
    "    except (KeyError, Exception) as e:\n",
    "        print(f\" - SKIP: {ticker} has no usable price column ({e})\")\n",
    "        failed_tickers.append({\"company\": company, \"ticker\": ticker, \"reason\": \"no_price_col\"})\n",
    "        continue\n",
    "\n",
    "    stock_df = stock_df.sort_values(\"date\").reset_index(drop=True)\n",
    "    stock_df[\"ipo_return\"] = stock_df[\"Adj_Close\"].pct_change()\n",
    "\n",
    "    # Align NIFTY slice for this stock window\n",
    "    nifty_slice = nifty_full[(nifty_full[\"date\"] >= stock_df[\"date\"].min()) & \n",
    "                             (nifty_full[\"date\"] <= stock_df[\"date\"].max())].copy()\n",
    "    if nifty_slice.empty:\n",
    "        print(f\" - SKIP: no overlapping NIFTY dates for {ticker}\")\n",
    "        failed_tickers.append({\"company\": company, \"ticker\": ticker, \"reason\": \"no_nifty_overlap\"})\n",
    "        continue\n",
    "\n",
    "    # Merge on date (left join so stock rows remain)\n",
    "    merged = pd.merge(stock_df, nifty_slice, on=\"date\", how=\"left\")\n",
    "\n",
    "    # Compute abnormal returns (if nifty_return missing, remains NaN)\n",
    "    merged[\"abnormal_return\"] = merged[\"ipo_return\"] - merged[\"nifty_return\"]\n",
    "\n",
    "    # Identify first trading date >= listing_date (listing_first_date)\n",
    "    trading_dates = merged[merged[\"date\"] >= listing_date]\n",
    "    if trading_dates.empty:\n",
    "        print(f\" - SKIP: {ticker} has no trading rows on/after listing date {listing_date}\")\n",
    "        failed_tickers.append({\"company\": company, \"ticker\": ticker, \"reason\": \"no_trading_after_listing\"})\n",
    "        continue\n",
    "    \n",
    "    listing_first_date = trading_dates[\"date\"].min()\n",
    "    merged[\"days_from_listing\"] = (merged[\"date\"] - listing_first_date).dt.days\n",
    "\n",
    "    # Compute CAR, CAR30, day1_return, vol30\n",
    "    merged[\"car\"] = merged[\"abnormal_return\"].fillna(0).cumsum()\n",
    "    car_30 = compute_car30(merged, listing_first_date)\n",
    "    day1_return = compute_day1_return(merged)\n",
    "    vol_30 = compute_vol30(merged)\n",
    "\n",
    "    # Add meta columns into merged daily df (so daily master is self-contained)\n",
    "    for c in ipo_meta.columns:\n",
    "        if c not in merged.columns:\n",
    "            merged[c] = meta_row.get(c, np.nan)\n",
    "\n",
    "    # Append to daily_rows and pooled summary\n",
    "    daily_rows.append(merged)\n",
    "    ipo_summary.append({\n",
    "        \"company_name\": company,\n",
    "        \"ticker\": ticker,\n",
    "        \"listing_date\": listing_date,\n",
    "        \"listing_first_date\": listing_first_date,\n",
    "        \"day1_return\": day1_return,\n",
    "        \"car_30\": car_30,\n",
    "        \"vol_30\": vol_30,\n",
    "        **{c: meta_row[c] for c in ipo_meta.columns if c not in [\"company_name\",\"ticker\",\"listing_date\"]}\n",
    "    })\n",
    "\n",
    "    print(f\" ✓ OK: {ticker} | CAR30={car_30:.4f} | vol30={vol_30 if not np.isnan(vol_30) else 'NaN'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a44408-c638-4426-baa6-9895a7b82fbd",
   "metadata": {},
   "source": [
    "4.10 Post Loop Assemebly and Save\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d5b1d5d8-b8ce-477a-bc4d-678dd9682f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "✓ SAVED OUTPUTS:\n",
      "============================================================\n",
      "Daily master: ..\\Notebooks\\IPO_Meta\\meta_output_processed.csv\\df_stock_master.csv (1014 rows)\n",
      "Pooled master: ..\\Notebooks\\IPO_Meta\\meta_output_processed.csv\\df_pooled_master.csv (40 IPOs)\n",
      "Sample 1 (High): ..\\Notebooks\\IPO_Meta\\meta_output_processed.csv\\sample1_final.csv (20 IPOs)\n",
      "Sample 2 (Low): ..\\Notebooks\\IPO_Meta\\meta_output_processed.csv\\sample2_final.csv (20 IPOs)\n"
     ]
    }
   ],
   "source": [
    "if len(daily_rows) ==0:\n",
    "    print(\"\\nNo daily rows collected. Nothing to save. Check metadata or fetch functions.\")\n",
    "else:\n",
    "    \n",
    "    df_stock_master = pd.concat(daily_rows, ignore_index=True)\n",
    "    df_pooled_master = pd.DataFrame(ipo_summary)\n",
    "\n",
    "    # Save both\n",
    "    df_stock_master.to_csv(OUT_DAILY_MASTER, index=False)\n",
    "    df_pooled_master.to_csv(OUT_POOLED_MASTER, index=False)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"✓ SAVED OUTPUTS:\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Daily master: {OUT_DAILY_MASTER} ({len(df_stock_master)} rows)\")\n",
    "    print(f\"Pooled master: {OUT_POOLED_MASTER} ({len(df_pooled_master)} IPOs)\")\n",
    "\n",
    "    # Create sample1 & sample2 based on subscription category (case-insensitive)\n",
    "    if \"subscription_cat\" in df_pooled_master.columns:\n",
    "        sample1_df = df_pooled_master[df_pooled_master[\"subscription_cat\"].str.lower() == \"high\"].reset_index(drop=True)\n",
    "        sample2_df = df_pooled_master[df_pooled_master[\"subscription_cat\"].str.lower() == \"low\"].reset_index(drop=True)\n",
    "        sample1_df.to_csv(OUT_SAMPLE1, index=False)\n",
    "        sample2_df.to_csv(OUT_SAMPLE2, index=False)\n",
    "        print(f\"Sample 1 (High): {OUT_SAMPLE1} ({len(sample1_df)} IPOs)\")\n",
    "        print(f\"Sample 2 (Low): {OUT_SAMPLE2} ({len(sample2_df)} IPOs)\")\n",
    "    else:\n",
    "        print(\"subscription_cat not present in pooled master; skipping sample split.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93a8181-baa2-48ba-a4d1-99b19ac8e3f8",
   "metadata": {},
   "source": [
    "4.11 Quick Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "06d41f95-ea79-4a19-b291-864325077a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DIAGNOSTICS:\n",
      "============================================================\n",
      "Failed tickers: 0\n",
      "✓ All tickers processed successfully!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DIAGNOSTICS:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Failed tickers: {len(failed_tickers)}\")\n",
    "if failed_tickers:\n",
    "    print(\"\\nFailed tickers breakdown:\")\n",
    "    print(pd.DataFrame(failed_tickers))\n",
    "else:\n",
    "    print(\"✓ All tickers processed successfully!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c78c52d0-ffd8-4850-b193-b2094d7a2153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip freeze > requirements.txt\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
